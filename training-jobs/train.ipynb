{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e78294-3a93-4f1b-8070-f8d2d138504b",
   "metadata": {},
   "source": [
    "# Text to Fashion Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a473fe8-ef6b-49a4-b7d2-d96600d9163d",
   "metadata": {},
   "source": [
    "## 1. Upload data to S3\n",
    "Here I use pokeman dataset as an example, which is composed of 833 image-text pairs. To scale up, you can just process your data into the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a3ee2c-be39-46cf-ae76-1ecfc52e0e24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f972fb3-9c30-4c5c-adea-36ed78e38d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4023f7-0ad4-460d-96f1-db5710f05d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e6697e-5363-4614-9371-284ec1b65fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prefix_data = 'datasets/zuimei-radar-cropped-debug'\n",
    "\n",
    "# local_data_path = \"../data/zuimei-radar-cropped-debug\"\n",
    "# input_data = sagemaker_session.upload_data(path=local_data_path, key_prefix=prefix_data)\n",
    "# print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183f2562-e9f2-415d-ad0e-4372fac14420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix_data = 'datasets/zuimei-radar-cropped-debug'\n",
    "input_data = 's3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31ccb1-e849-43e0-85eb-9dd8b8984fa5",
   "metadata": {},
   "source": [
    "## 2. Upload pretrained models to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35dcff89-e473-497b-bc7a-4e333fbcc706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prefix_model = 'models/dgmr_all'\n",
    "\n",
    "# local_model_path = \"../models\"\n",
    "# input_model = sagemaker_session.upload_data(path=local_model_path, key_prefix=prefix_model)\n",
    "# print(input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c4af391-0ae0-42af-993a-c68fd48ef4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_model = \"s3://sagemaker-us-west-2-452145973879/models/dgmr_all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bb61d-6527-4707-9cbb-185c89b51008",
   "metadata": {},
   "source": [
    "## 2. Start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a06ebc-ab38-48b6-9e9e-9c3f0145440e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: dgmr-launch-2024-05-14-05-20-05-747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-14 05:20:08 Starting - Starting the training job...\n",
      "2024-05-14 05:20:16 Pending - Training job waiting for capacity...\n",
      "2024-05-14 05:20:50 Pending - Preparing the instances for training......\n",
      "2024-05-14 05:21:53 Downloading - Downloading input data......\n",
      "2024-05-14 05:22:43 Downloading - Downloading the training image.........\n",
      "2024-05-14 05:24:29 Training - Training image download completed. Training in progress........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:40,391 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:40,453 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:40,464 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:40,466 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:42,204 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[34mCollecting antialiased_cnns (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading antialiased_cnns-0.3-py3-none-any.whl.metadata (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch_msssim (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.17.0)\u001b[0m\n",
      "\u001b[34mCollecting pytorch_lightning (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-2.2.4-py3-none-any.whl.metadata (21 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.7.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface_hub==0.20.0 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting cartopy (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading Cartopy-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting webdataset (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading webdataset-0.2.86-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (3.13.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (2024.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.11.0->-r requirements.txt (line 5)) (10.3.0)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics>=0.7.0 (from pytorch_lightning->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-1.4.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting lightning-utilities>=0.8.0 (from pytorch_lightning->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (4.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (5.9.8)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.1.1-py2.py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (68.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (15.0.2)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (2.2.2)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (0.70.16)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec>=2023.5.0 (from huggingface_hub==0.20.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.5 in /opt/conda/lib/python3.10/site-packages (from cartopy->-r requirements.txt (line 11)) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting shapely>=1.7 (from cartopy->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading shapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyshp>=2.3 (from cartopy->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading pyshp-2.3.1-py2.py3-none-any.whl.metadata (55 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.0/56.0 kB 9.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyproj>=3.3.1 (from cartopy->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading pyproj-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting braceexpand (from webdataset->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 9)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pyproj>=3.3.1->cartopy->-r requirements.txt (line 11)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub==0.20.0->-r requirements.txt (line 8)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub==0.20.0->-r requirements.txt (line 8)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub==0.20.0->-r requirements.txt (line 8)) (1.26.18)\u001b[0m\n",
      "\u001b[34mCollecting pretty-errors==1.2.25 (from torchmetrics>=0.7.0->pytorch_lightning->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading pretty_errors-1.2.25-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from pretty-errors==1.2.25->torchmetrics>=0.7.0->pytorch_lightning->-r requirements.txt (line 6)) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 10)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 10)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.20.0-py3-none-any.whl (329 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 329.1/329.1 kB 16.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading antialiased_cnns-0.3-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mDownloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-2.2.4-py3-none-any.whl (802 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 802.2/802.2 kB 43.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 107.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 46.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading Cartopy-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 109.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading webdataset-0.2.86-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.4/70.4 kB 11.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.9/170.9 kB 27.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 73.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 32.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mDownloading pyproj-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.3/8.3 MB 106.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyshp-2.3.1-py2.py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.5/46.5 kB 8.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.1.1-py2.py3-none-any.whl (277 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 277.3/277.3 kB 31.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading shapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 90.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-1.4.0-py3-none-any.whl (868 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 868.8/868.8 kB 71.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pretty_errors-1.2.25-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 21.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 27.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 10.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 13.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 36.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: braceexpand, antialiased_cnns, xxhash, webdataset, smmap, shapely, setproctitle, sentry-sdk, pyshp, pyproj, pyarrow-hotfix, pretty-errors, multidict, lightning-utilities, fsspec, frozenlist, docker-pycreds, async-timeout, yarl, huggingface_hub, gitdb, aiosignal, torchmetrics, pytorch_msssim, gitpython, cartopy, aiohttp, wandb, pytorch_lightning, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2024.3.1\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2024.3.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2024.3.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.9.5 aiosignal-1.3.1 antialiased_cnns-0.3 async-timeout-4.0.3 braceexpand-0.1.7 cartopy-0.23.0 datasets-2.18.0 docker-pycreds-0.4.0 frozenlist-1.4.1 fsspec-2024.2.0 gitdb-4.0.11 gitpython-3.1.43 huggingface_hub-0.20.0 lightning-utilities-0.11.2 multidict-6.0.5 pretty-errors-1.2.25 pyarrow-hotfix-0.6 pyproj-3.6.1 pyshp-2.3.1 pytorch_lightning-2.2.4 pytorch_msssim-1.0.0 sentry-sdk-2.1.1 setproctitle-1.3.3 shapely-2.0.4 smmap-5.0.1 torchmetrics-1.4.0 wandb-0.17.0 webdataset-0.2.86 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:53,745 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:53,746 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:53,831 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:53,908 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:53,984 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:53,997 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"dgmr-launch-2024-05-14-05-20-05-747\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-452145973879/dgmr-launch-2024-05-14-05-20-05-747/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entry.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.48xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entry\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-452145973879/dgmr-launch-2024-05-14-05-20-05-747/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"dgmr-launch-2024-05-14-05-20-05-747\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-452145973879/dgmr-launch-2024-05-14-05-20-05-747/source/sourcedir.tar.gz\",\"module_name\":\"entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 entry.py\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:53,998 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-05-14 05:25:53,998 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mCloning into 'skillful_nowcasting'...\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: antialiased_cnns in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch_msssim in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (2.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface_hub==0.20.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.20.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cartopy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: webdataset in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.2.86)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (3.13.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.20.0->-r requirements.txt (line 8)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.11.0->-r requirements.txt (line 5)) (10.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 6)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 6)) (0.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (8.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (0.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (3.1.43)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (4.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (68.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (15.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 10)) (3.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.5 in /opt/conda/lib/python3.10/site-packages (from cartopy->-r requirements.txt (line 11)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shapely>=1.7 in /opt/conda/lib/python3.10/site-packages (from cartopy->-r requirements.txt (line 11)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyshp>=2.3 in /opt/conda/lib/python3.10/site-packages (from cartopy->-r requirements.txt (line 11)) (2.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyproj>=3.3.1 in /opt/conda/lib/python3.10/site-packages (from cartopy->-r requirements.txt (line 11)) (3.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: braceexpand in /opt/conda/lib/python3.10/site-packages (from webdataset->-r requirements.txt (line 12)) (0.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 9)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (4.0.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.5->cartopy->-r requirements.txt (line 11)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pyproj>=3.3.1->cartopy->-r requirements.txt (line 11)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub==0.20.0->-r requirements.txt (line 8)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub==0.20.0->-r requirements.txt (line 8)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub==0.20.0->-r requirements.txt (line 8)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pretty-errors==1.2.25 in /opt/conda/lib/python3.10/site-packages (from torchmetrics>=0.7.0->pytorch_lightning->-r requirements.txt (line 6)) (1.2.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from pretty-errors==1.2.25->torchmetrics>=0.7.0->pytorch_lightning->-r requirements.txt (line 6)) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 10)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 10)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (5.0.1)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mObtaining file:///opt/ml/code/skillful_nowcasting\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from dgmr==1.3.4) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: antialiased_cnns in /opt/conda/lib/python3.10/site-packages (from dgmr==1.3.4) (0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch_msssim in /opt/conda/lib/python3.10/site-packages (from dgmr==1.3.4) (1.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from dgmr==1.3.4) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from dgmr==1.3.4) (0.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (from dgmr==1.3.4) (2.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from dgmr==1.3.4) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from dgmr==1.3.4) (0.20.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.11.0->dgmr==1.3.4) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.11.0->dgmr==1.3.4) (10.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->dgmr==1.3.4) (3.13.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->dgmr==1.3.4) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->dgmr==1.3.4) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->dgmr==1.3.4) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->dgmr==1.3.4) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->dgmr==1.3.4) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning->dgmr==1.3.4) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning->dgmr==1.3.4) (0.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->dgmr==1.3.4) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->dgmr==1.3.4) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->dgmr==1.3.4) (3.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning->dgmr==1.3.4) (3.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch_lightning->dgmr==1.3.4) (68.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pretty-errors==1.2.25 in /opt/conda/lib/python3.10/site-packages (from torchmetrics>=0.7.0->pytorch_lightning->dgmr==1.3.4) (1.2.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from pretty-errors==1.2.25->torchmetrics>=0.7.0->pytorch_lightning->dgmr==1.3.4) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->dgmr==1.3.4) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.11.0->dgmr==1.3.4) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.11.0->dgmr==1.3.4) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.11.0->dgmr==1.3.4) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.11.0->dgmr==1.3.4) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->dgmr==1.3.4) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->dgmr==1.3.4) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->dgmr==1.3.4) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->dgmr==1.3.4) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->dgmr==1.3.4) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->dgmr==1.3.4) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->dgmr==1.3.4) (4.0.3)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: dgmr\u001b[0m\n",
      "\u001b[34mRunning setup.py develop for dgmr\u001b[0m\n",
      "\u001b[34mSuccessfully installed dgmr-1.3.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mwandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: zhangchvic (genai-view). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.17.0\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20240514_052606-dgmr-launch-2024-05-14-05-20-05-747-ukhysi-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run dgmr-launch-2024-05-14-05-20-05-747-ukhysi-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-ukhysi-algo-1\u001b[0m\n",
      "\u001b[34m*****************start cp data and pretrained models*****************************\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/train/000002.tar data/zuimei-radar-cropped/train/000002.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/train/000003.tar data/zuimei-radar-cropped/train/000003.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/train/000005.tar data/zuimei-radar-cropped/train/000005.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/train/000009.tar data/zuimei-radar-cropped/train/000009.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/train/000001.tar data/zuimei-radar-cropped/train/000001.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/train/000000.tar data/zuimei-radar-cropped/train/000000.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/train/000007.tar data/zuimei-radar-cropped/train/000007.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/train/000008.tar data/zuimei-radar-cropped/train/000008.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/train/000004.tar data/zuimei-radar-cropped/train/000004.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/train/000006.tar data/zuimei-radar-cropped/train/000006.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/valid/000518.tar data/zuimei-radar-cropped/valid/000518.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/valid/000519.tar data/zuimei-radar-cropped/valid/000519.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/valid/000520.tar data/zuimei-radar-cropped/valid/000520.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/zuimei-radar-cropped-debug/valid/000521.tar data/zuimei-radar-cropped/valid/000521.tar\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/HEAD models/dgmr/.git/HEAD\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/pre-rebase.sample models/dgmr/.git/hooks/pre-rebase.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/config models/dgmr/.git/config\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/post-update.sample models/dgmr/.git/hooks/post-update.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/description models/dgmr/.git/description\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/pre-applypatch.sample models/dgmr/.git/hooks/pre-applypatch.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/pre-merge-commit.sample models/dgmr/.git/hooks/pre-merge-commit.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/post-commit models/dgmr/.git/hooks/post-commit\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/post-merge models/dgmr/.git/hooks/post-merge\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/fsmonitor-watchman.sample models/dgmr/.git/hooks/fsmonitor-watchman.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/applypatch-msg.sample models/dgmr/.git/hooks/applypatch-msg.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/prepare-commit-msg.sample models/dgmr/.git/hooks/prepare-commit-msg.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/pre-push models/dgmr/.git/hooks/pre-push\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/info/exclude models/dgmr/.git/info/exclude\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/0c/3a3c6b34abb27ce503ad99f2104ae3af50010b models/dgmr/.git/objects/0c/3a3c6b34abb27ce503ad99f2104ae3af50010b\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/21/b29ab864793dc86d157902941b2ff4bbe2bbca models/dgmr/.git/objects/21/b29ab864793dc86d157902941b2ff4bbe2bbca\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/51/2dc7765afe096769c079c1a96092e4c2c96e8f models/dgmr/.git/objects/51/2dc7765afe096769c079c1a96092e4c2c96e8f\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/27/e0fb0e7b8689c5f0b845fe943e2be97884b836 models/dgmr/.git/objects/27/e0fb0e7b8689c5f0b845fe943e2be97884b836\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/commit-msg.sample models/dgmr/.git/hooks/commit-msg.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/pre-commit.sample models/dgmr/.git/hooks/pre-commit.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/5b/04b0f7bc3bcd005bad12d9da88d174442eb5ee models/dgmr/.git/objects/5b/04b0f7bc3bcd005bad12d9da88d174442eb5ee\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/ee/7b291e20d22a584f178c0ad99aa94e12dec3a8 models/dgmr/.git/objects/ee/7b291e20d22a584f178c0ad99aa94e12dec3a8\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/6b/6c4f569ce1b77a0920c6b886aafbebfe3c442f models/dgmr/.git/objects/6b/6c4f569ce1b77a0920c6b886aafbebfe3c442f\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/f9/dddeba9d1cc9d557cd90c528430bbe3ee38ac8 models/dgmr/.git/objects/f9/dddeba9d1cc9d557cd90c528430bbe3ee38ac8\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/logs/refs/heads/main models/dgmr/.git/logs/refs/heads/main\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/81/5a9bf0502f5c828078f9013132a1f19a1a779b models/dgmr/.git/objects/81/5a9bf0502f5c828078f9013132a1f19a1a779b\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/update.sample models/dgmr/.git/hooks/update.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/refs/remotes/origin/HEAD models/dgmr/.git/refs/remotes/origin/HEAD\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/64/ec8cfc3e1bd0e3b7bc1b88f6df4630dbd4570b models/dgmr/.git/objects/64/ec8cfc3e1bd0e3b7bc1b88f6df4630dbd4570b\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/58/1c1b164317c44d593d90b29dbd737ea7f18b45 models/dgmr/.git/objects/58/1c1b164317c44d593d90b29dbd737ea7f18b45\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/53/cb4e62a391c02600a02a774b52abc0c224f50e models/dgmr/.git/objects/53/cb4e62a391c02600a02a774b52abc0c224f50e\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/42/ca53d0ec187eb3cd74b7b045c08de302d3715f models/dgmr/.git/objects/42/ca53d0ec187eb3cd74b7b045c08de302d3715f\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/post-checkout models/dgmr/.git/hooks/post-checkout\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/logs/HEAD models/dgmr/.git/logs/HEAD\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/91/37fc6e6b65af52b7f22cc11ecdd976eccccac5 models/dgmr/.git/objects/91/37fc6e6b65af52b7f22cc11ecdd976eccccac5\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/af/3ebd96bfe4d3f5e427feff7aac71748044e086 models/dgmr/.git/objects/af/3ebd96bfe4d3f5e427feff7aac71748044e086\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/logs/refs/remotes/origin/HEAD models/dgmr/.git/logs/refs/remotes/origin/HEAD\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/72/6de975188e9e71623bde5ac28dbab8e8f5d709 models/dgmr/.git/objects/72/6de975188e9e71623bde5ac28dbab8e8f5d709\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/c5/488d13eddd0037c88a436edc83432a9f77fa50 models/dgmr/.git/objects/c5/488d13eddd0037c88a436edc83432a9f77fa50\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/97/300d85a12cd01b9e69fcff3125e6b831e4e07c models/dgmr/.git/objects/97/300d85a12cd01b9e69fcff3125e6b831e4e07c\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/f5/7d3fc8a1d2d920da8f3e2b14e5431ab8a27109 models/dgmr/.git/objects/f5/7d3fc8a1d2d920da8f3e2b14e5431ab8a27109\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/index models/dgmr/.git/index\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/packed-refs models/dgmr/.git/packed-refs\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/pre-push.sample models/dgmr/.git/hooks/pre-push.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/push-to-checkout.sample models/dgmr/.git/hooks/push-to-checkout.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.gitattributes models/dgmr/.gitattributes\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/refs/heads/main models/dgmr/.git/refs/heads/main\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/0e/7747a87f69b6ce728c8ee9108f746e9cd6a280 models/dgmr/.git/objects/0e/7747a87f69b6ce728c8ee9108f746e9cd6a280\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/7e/66d258705e5f02730abe6d2ca6c3e059fb9c32 models/dgmr/.git/objects/7e/66d258705e5f02730abe6d2ca6c3e059fb9c32\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/README.md models/dgmr/README.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.ipynb_checkpoints/config-checkpoint.json models/dgmr/.ipynb_checkpoints/config-checkpoint.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/6d/34772f5ca361021038b404fb913ec8dc0b1a5a models/dgmr/.git/objects/6d/34772f5ca361021038b404fb913ec8dc0b1a5a\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/hooks/pre-receive.sample models/dgmr/.git/hooks/pre-receive.sample\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/config.json models/dgmr/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.ipynb_checkpoints/README-checkpoint.md models/dgmr/.ipynb_checkpoints/README-checkpoint.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/objects/ed/3b9f15ba12cca94722575b56182cba755435b6 models/dgmr/.git/objects/ed/3b9f15ba12cca94722575b56182cba755435b6\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/pytorch_model.bin models/dgmr/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/dgmr_all/dgmr/.git/lfs/objects/89/e9/89e9b42bf63cb449d4d2274704e540e1a7c6d61aa264376fd6a7129db1444fe8 models/dgmr/.git/lfs/objects/89/e9/89e9b42bf63cb449d4d2274704e540e1a7c6d61aa264376fd6a7129db1444fe8\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 100\u001b[0m\n",
      "\u001b[34mNum batches each epoch = 12\u001b[0m\n",
      "\u001b[34mNum Epochs = 5\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 60\u001b[0m\n",
      "\u001b[34mUsing bfloat16 Automatic Mixed Precision (AMP)\u001b[0m\n",
      "\u001b[34mGPU available: True (cuda), used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34mLoading weights from local directory\u001b[0m\n",
      "\u001b[34m------model.config parameters------\u001b[0m\n",
      "\u001b[34mforecast_steps: 18\u001b[0m\n",
      "\u001b[34mgeneration_steps: 6\u001b[0m\n",
      "\u001b[34mYou are using a CUDA device ('NVIDIA A10G') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: zhangchvic (genai-view). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: zhangchvic (genai-view). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: zhangchvic (genai-view). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: zhangchvic (genai-view). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: zhangchvic (genai-view). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: zhangchvic (genai-view). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: zhangchvic (genai-view). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.17.0\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-0j9v6w-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run dgmr-launch-2024-05-14-05-20-05-747-0j9v6w-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-0j9v6w-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.17.0\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-8pv7l6-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.17.0\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-nt8f0n-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run dgmr-launch-2024-05-14-05-20-05-747-8pv7l6-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-8pv7l6-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Syncing run dgmr-launch-2024-05-14-05-20-05-747-nt8f0n-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-nt8f0n-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.17.0\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-ed9105-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run dgmr-launch-2024-05-14-05-20-05-747-ed9105-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-ed9105-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.17.0\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-i9b66k-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run dgmr-launch-2024-05-14-05-20-05-747-i9b66k-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-i9b66k-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.17.0\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-rf8x5j-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run dgmr-launch-2024-05-14-05-20-05-747-rf8x5j-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-rf8x5j-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.17.0\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-v9n8q9-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run dgmr-launch-2024-05-14-05-20-05-747-v9n8q9-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-v9n8q9-algo-1\u001b[0m\n",
      "\u001b[34m*****************start cp data and pretrained models*****************************\u001b[0m\n",
      "\u001b[34m*****************start cp data and pretrained models*****************************\u001b[0m\n",
      "\u001b[34m*****************start cp data and pretrained models*****************************\u001b[0m\n",
      "\u001b[34m*****************start cp data and pretrained models*****************************\u001b[0m\n",
      "\u001b[34m*****************start cp data and pretrained models*****************************\u001b[0m\n",
      "\u001b[34m*****************start cp data and pretrained models*****************************\u001b[0m\n",
      "\u001b[34m*****************start cp data and pretrained models*****************************\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 100\u001b[0m\n",
      "\u001b[34mNum batches each epoch = 12\u001b[0m\n",
      "\u001b[34mNum Epochs = 5\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 60\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 100\u001b[0m\n",
      "\u001b[34mNum batches each epoch = 12\u001b[0m\n",
      "\u001b[34mNum Epochs = 5\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 60\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 100\u001b[0m\n",
      "\u001b[34mNum batches each epoch = 12\u001b[0m\n",
      "\u001b[34mNum Epochs = 5\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 60\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 100\u001b[0m\n",
      "\u001b[34mNum batches each epoch = 12\u001b[0m\n",
      "\u001b[34mNum Epochs = 5\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 60\u001b[0m\n",
      "\u001b[34m***** Running training ********** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 100\u001b[0m\n",
      "\u001b[34mNum examples = 100\u001b[0m\n",
      "\u001b[34mNum batches each epoch = 12\u001b[0m\n",
      "\u001b[34mNum batches each epoch = 12\u001b[0m\n",
      "\u001b[34mNum Epochs = 5\u001b[0m\n",
      "\u001b[34mNum Epochs = 5\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 1  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8  Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 60\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 60\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 100\u001b[0m\n",
      "\u001b[34mNum batches each epoch = 12\u001b[0m\n",
      "\u001b[34mNum Epochs = 5\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 60\u001b[0m\n",
      "\u001b[34mLoading weights from local directory\u001b[0m\n",
      "\u001b[34mLoading weights from local directory\u001b[0m\n",
      "\u001b[34mLoading weights from local directory\u001b[0m\n",
      "\u001b[34mLoading weights from local directory\u001b[0m\n",
      "\u001b[34mLoading weights from local directory\u001b[0m\n",
      "\u001b[34mLoading weights from local directory\u001b[0m\n",
      "\u001b[34mLoading weights from local directory\u001b[0m\n",
      "\u001b[34m------model.config parameters------\u001b[0m\n",
      "\u001b[34mforecast_steps: 18\u001b[0m\n",
      "\u001b[34mgeneration_steps: 6\u001b[0m\n",
      "\u001b[34m------model.config parameters------\u001b[0m\n",
      "\u001b[34mforecast_steps: 18\u001b[0m\n",
      "\u001b[34mgeneration_steps: 6\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\u001b[0m\n",
      "\u001b[34m------model.config parameters------\u001b[0m\n",
      "\u001b[34mforecast_steps: 18\u001b[0m\n",
      "\u001b[34mgeneration_steps: 6\u001b[0m\n",
      "\u001b[34m------model.config parameters------\u001b[0m\n",
      "\u001b[34mforecast_steps: 18\u001b[0m\n",
      "\u001b[34mgeneration_steps: 6\u001b[0m\n",
      "\u001b[34m------model.config parameters------\u001b[0m\n",
      "\u001b[34mforecast_steps: 18\u001b[0m\n",
      "\u001b[34mgeneration_steps: 6\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\u001b[0m\n",
      "\u001b[34m------model.config parameters------\u001b[0m\n",
      "\u001b[34mforecast_steps: 18\u001b[0m\n",
      "\u001b[34mgeneration_steps: 6\u001b[0m\n",
      "\u001b[34m------model.config parameters------\u001b[0m\n",
      "\u001b[34mforecast_steps: 18\u001b[0m\n",
      "\u001b[34mgeneration_steps: 6\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed_backend=nccl\u001b[0m\n",
      "\u001b[34mAll distributed processes registered. Starting with 8 processes\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34malgo-1:96:96 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:96:96 [0] NCCL INFO Bootstrap : Using eth0:10.0.206.211<0>\u001b[0m\n",
      "\u001b[34malgo-1:96:96 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:96:96 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:96:96 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:373:373 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:370:370 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:371:371 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:372:372 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:373:373 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:375:375 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:374:374 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:370:370 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:371:371 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:372:372 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:375:375 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:374:374 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:376:376 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:376:376 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:370:370 [1] NCCL INFO Bootstrap : Using eth0:10.0.206.211<0>\u001b[0m\n",
      "\u001b[34malgo-1:373:373 [4] NCCL INFO Bootstrap : Using eth0:10.0.206.211<0>\u001b[0m\n",
      "\u001b[34malgo-1:372:372 [3] NCCL INFO Bootstrap : Using eth0:10.0.206.211<0>\u001b[0m\n",
      "\u001b[34malgo-1:375:375 [6] NCCL INFO Bootstrap : Using eth0:10.0.206.211<0>\u001b[0m\n",
      "\u001b[34malgo-1:374:374 [5] NCCL INFO Bootstrap : Using eth0:10.0.206.211<0>\u001b[0m\n",
      "\u001b[34malgo-1:376:376 [7] NCCL INFO Bootstrap : Using eth0:10.0.206.211<0>\u001b[0m\n",
      "\u001b[34malgo-1:371:371 [2] NCCL INFO Bootstrap : Using eth0:10.0.206.211<0>\u001b[0m\n",
      "\u001b[34malgo-1:372:372 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:372:372 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:373:373 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:373:373 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:375:375 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:375:375 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:371:371 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:371:371 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:370:370 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:370:370 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:376:376 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:376:376 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:374:374 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:374:374 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.0-aws\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO comm 0x56024f4ca4f0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 170 commId 0xb942c2d00bbb948f - Init START\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO comm 0x55f83426b090 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 190 commId 0xb942c2d00bbb948f - Init START\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO comm 0x5597b3a9eb00 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 180 commId 0xb942c2d00bbb948f - Init START\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO comm 0x55c4a2bc6990 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 1b0 commId 0xb942c2d00bbb948f - Init START\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO comm 0x55b39cfaa7b0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId 1d0 commId 0xb942c2d00bbb948f - Init START\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO comm 0x556ea2308e90 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId 1c0 commId 0xb942c2d00bbb948f - Init START\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO comm 0x5577d2ddcd20 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 1a0 commId 0xb942c2d00bbb948f - Init START\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO comm 0x558c77749c20 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 160 commId 0xb942c2d00bbb948f - Init START\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NVLS multicast support is not available on dev 6\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NVLS multicast support is not available on dev 5\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NVLS multicast support is not available on dev 4\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO Setting affinity for GPU 3 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NVLS multicast support is not available on dev 7\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NVLS multicast support is not available on dev 3\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO Setting affinity for GPU 2 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NVLS multicast support is not available on dev 0\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NVLS multicast support is not available on dev 1\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NVLS multicast support is not available on dev 2\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO Channel 00 : 5[5] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO Channel 01 : 5[5] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO Channel 00 : 4[4] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO Channel 01 : 4[4] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO Channel 00 : 6[6] -> 7[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO Channel 00 : 7[7] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO Channel 01 : 6[6] -> 7[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO Channel 01 : 7[7] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO Channel 00 : 7[7] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO Channel 01 : 7[7] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO Channel 00 : 6[6] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO Channel 00 : 5[5] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO Channel 00 : 3[3] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO Channel 01 : 6[6] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO Channel 01 : 5[5] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:375:2509 [6] NCCL INFO comm 0x556ea2308e90 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId 1c0 commId 0xb942c2d00bbb948f - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:371:2507 [2] NCCL INFO comm 0x5597b3a9eb00 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 180 commId 0xb942c2d00bbb948f - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:374:2510 [5] NCCL INFO comm 0x55c4a2bc6990 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 1b0 commId 0xb942c2d00bbb948f - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:370:2508 [1] NCCL INFO comm 0x56024f4ca4f0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 170 commId 0xb942c2d00bbb948f - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:372:2505 [3] NCCL INFO comm 0x55f83426b090 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 190 commId 0xb942c2d00bbb948f - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:376:2511 [7] NCCL INFO comm 0x55b39cfaa7b0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId 1d0 commId 0xb942c2d00bbb948f - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:373:2506 [4] NCCL INFO comm 0x5577d2ddcd20 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 1a0 commId 0xb942c2d00bbb948f - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:96:2504 [0] NCCL INFO comm 0x558c77749c20 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 160 commId 0xb942c2d00bbb948f - Init COMPLETE\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /opt/ml/code/checkpoint/dgmr_forecast18_ep50_max_nonzero0.9 exists and is not empty.\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34m| Name               | Type                     | Params\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | discriminator_loss | NowcastingLoss           | 0     \u001b[0m\n",
      "\u001b[34m1 | grid_regularizer   | GridCellLoss             | 0     \u001b[0m\n",
      "\u001b[34m2 | conditioning_stack | ContextConditioningStack | 4.2 M \u001b[0m\n",
      "\u001b[34m3 | latent_stack       | LatentConditioningStack  | 7.2 M \u001b[0m\n",
      "\u001b[34m4 | sampler            | Sampler                  | 41.3 M\u001b[0m\n",
      "\u001b[34m5 | generator          | Generator                | 52.8 M\u001b[0m\n",
      "\u001b[34m6 | discriminator      | Discriminator            | 43.5 M\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m96.3 M    Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m96.3 M    Total params\u001b[0m\n",
      "\u001b[34m385.222   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34mSanity Checking: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mSanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0:  50%|█████     | 1/2 [00:03<00:03,  0.32it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0: 100%|██████████| 2/2 [00:05<00:00,  0.36it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/d_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/g_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/grid_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\u001b[0m\n",
      "\u001b[34mTraining: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   8%|▊         | 1/12 [00:11<02:01,  0.09it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   8%|▊         | 1/12 [00:11<02:01,  0.09it/s, v_num=go-1, train/d_loss=74.00, train/g_loss=1.48e+3, train/grid_loss=70.20, global_step=1.000]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 2/12 [00:18<01:34,  0.11it/s, v_num=go-1, train/d_loss=74.00, train/g_loss=1.48e+3, train/grid_loss=70.20, global_step=1.000]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 2/12 [00:18<01:34,  0.11it/s, v_num=go-1, train/d_loss=28.20, train/g_loss=274.0, train/grid_loss=10.90, global_step=2.000]\u001b[0m\n",
      "\u001b[34mEpoch 0:  25%|██▌       | 3/12 [00:26<01:19,  0.11it/s, v_num=go-1, train/d_loss=28.20, train/g_loss=274.0, train/grid_loss=10.90, global_step=2.000]\u001b[0m\n",
      "\u001b[34mEpoch 0:  25%|██▌       | 3/12 [00:26<01:19,  0.11it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=140.0, train/grid_loss=5.210, global_step=3.000]\u001b[0m\n",
      "\u001b[34mEpoch 0:  33%|███▎      | 4/12 [00:34<01:08,  0.12it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=140.0, train/grid_loss=5.210, global_step=3.000]\u001b[0m\n",
      "\u001b[34mEpoch 0:  33%|███▎      | 4/12 [00:34<01:08,  0.12it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=205.0, train/grid_loss=9.470, global_step=4.000]\u001b[0m\n",
      "\u001b[34mEpoch 0:  42%|████▏     | 5/12 [00:41<00:58,  0.12it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=205.0, train/grid_loss=9.470, global_step=4.000]\u001b[0m\n",
      "\u001b[34mEpoch 0:  42%|████▏     | 5/12 [00:41<00:58,  0.12it/s, v_num=go-1, train/d_loss=0.00989, train/g_loss=14.40, train/grid_loss=1.270, global_step=5.000]\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mValidation: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  20%|██        | 1/5 [00:02<00:10,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  40%|████      | 2/5 [00:05<00:07,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  80%|████████  | 4/5 [00:10<00:02,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 5/5 [00:12<00:00,  0.40it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:  42%|████▏     | 5/12 [00:54<01:16,  0.09it/s, v_num=go-1, train/d_loss=0.00989, train/g_loss=14.40, train/grid_loss=1.270, global_step=5.000, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  50%|█████     | 6/12 [01:02<01:02,  0.10it/s, v_num=go-1, train/d_loss=0.00989, train/g_loss=14.40, train/grid_loss=1.270, global_step=5.000, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  50%|█████     | 6/12 [01:02<01:02,  0.10it/s, v_num=go-1, train/d_loss=29.30, train/g_loss=210.0, train/grid_loss=9.250, global_step=6.000, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  58%|█████▊    | 7/12 [01:10<00:50,  0.10it/s, v_num=go-1, train/d_loss=29.30, train/g_loss=210.0, train/grid_loss=9.250, global_step=6.000, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  58%|█████▊    | 7/12 [01:10<00:50,  0.10it/s, v_num=go-1, train/d_loss=91.60, train/g_loss=1.19e+3, train/grid_loss=56.00, global_step=7.000, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  67%|██████▋   | 8/12 [01:18<00:39,  0.10it/s, v_num=go-1, train/d_loss=91.60, train/g_loss=1.19e+3, train/grid_loss=56.00, global_step=7.000, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  67%|██████▋   | 8/12 [01:18<00:39,  0.10it/s, v_num=go-1, train/d_loss=0.105, train/g_loss=166.0, train/grid_loss=7.150, global_step=8.000, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  75%|███████▌  | 9/12 [01:25<00:28,  0.10it/s, v_num=go-1, train/d_loss=0.105, train/g_loss=166.0, train/grid_loss=7.150, global_step=8.000, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  75%|███████▌  | 9/12 [01:25<00:28,  0.10it/s, v_num=go-1, train/d_loss=62.30, train/g_loss=1.09e+3, train/grid_loss=53.30, global_step=9.000, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 10/12 [01:33<00:18,  0.11it/s, v_num=go-1, train/d_loss=62.30, train/g_loss=1.09e+3, train/grid_loss=53.30, global_step=9.000, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 10/12 [01:33<00:18,  0.11it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=147.0, train/grid_loss=4.340, global_step=10.00, val/d_loss=70.10, val/g_loss=353.0, val/grid_loss=19.10]\u001b[0m\n",
      "\u001b[34mcp checkpoint/dgmr_forecast18_ep50_max_nonzero0.9/args.json s3://sagemaker-us-west-2-452145973879/checkpoints/dgmr_checkpoint05-14-2024-05-26-25/dgmr_forecast18_ep50_max_nonzero0.9/args.json\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mValidation: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  20%|██        | 1/5 [00:02<00:10,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  40%|████      | 2/5 [00:05<00:07,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  80%|████████  | 4/5 [00:10<00:02,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 5/5 [00:12<00:00,  0.39it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 10/12 [01:49<00:21,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=147.0, train/grid_loss=4.340, global_step=10.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 0:  92%|█████████▏| 11/12 [01:57<00:10,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=147.0, train/grid_loss=4.340, global_step=10.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 0:  92%|█████████▏| 11/12 [01:57<00:10,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=120.0, train/grid_loss=3.780, global_step=11.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 12/12 [02:04<00:00,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=120.0, train/grid_loss=3.780, global_step=11.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 12/12 [02:04<00:00,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=56.30, train/grid_loss=1.810, global_step=12.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 12/12 [02:04<00:00,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=56.30, train/grid_loss=1.810, global_step=12.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/12 [00:00<?, ?it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=56.30, train/grid_loss=1.810, global_step=12.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:   0%|          | 0/12 [00:00<?, ?it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=56.30, train/grid_loss=1.810, global_step=12.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:   8%|▊         | 1/12 [00:08<01:31,  0.12it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=56.30, train/grid_loss=1.810, global_step=12.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:   8%|▊         | 1/12 [00:08<01:31,  0.12it/s, v_num=go-1, train/d_loss=64.60, train/g_loss=946.0, train/grid_loss=43.30, global_step=13.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 2/12 [00:15<01:19,  0.13it/s, v_num=go-1, train/d_loss=64.60, train/g_loss=946.0, train/grid_loss=43.30, global_step=13.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 2/12 [00:15<01:19,  0.13it/s, v_num=go-1, train/d_loss=11.60, train/g_loss=158.0, train/grid_loss=7.140, global_step=14.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:  25%|██▌       | 3/12 [00:23<01:10,  0.13it/s, v_num=go-1, train/d_loss=11.60, train/g_loss=158.0, train/grid_loss=7.140, global_step=14.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:  25%|██▌       | 3/12 [00:23<01:10,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=117.0, train/grid_loss=4.430, global_step=15.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:  33%|███▎      | 4/12 [00:31<01:03,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=117.0, train/grid_loss=4.430, global_step=15.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:  33%|███▎      | 4/12 [00:31<01:03,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=163.0, train/grid_loss=7.360, global_step=16.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:  42%|████▏     | 5/12 [00:39<00:54,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=163.0, train/grid_loss=7.360, global_step=16.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mEpoch 1:  42%|████▏     | 5/12 [00:39<00:54,  0.13it/s, v_num=go-1, train/d_loss=21.40, train/g_loss=49.80, train/grid_loss=1.670, global_step=17.00, val/d_loss=7.820, val/g_loss=41.70, val/grid_loss=1.720]\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mValidation: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  20%|██        | 1/5 [00:02<00:10,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  40%|████      | 2/5 [00:05<00:07,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  80%|████████  | 4/5 [00:10<00:02,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 5/5 [00:12<00:00,  0.40it/s]\u001b[0m\n",
      "\u001b[34mEpoch 1:  42%|████▏     | 5/12 [00:52<01:13,  0.10it/s, v_num=go-1, train/d_loss=21.40, train/g_loss=49.80, train/grid_loss=1.670, global_step=17.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mEpoch 1:  50%|█████     | 6/12 [01:00<01:00,  0.10it/s, v_num=go-1, train/d_loss=21.40, train/g_loss=49.80, train/grid_loss=1.670, global_step=17.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mEpoch 1:  50%|█████     | 6/12 [01:00<01:00,  0.10it/s, v_num=go-1, train/d_loss=17.60, train/g_loss=107.0, train/grid_loss=5.630, global_step=18.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mEpoch 1:  58%|█████▊    | 7/12 [01:07<00:48,  0.10it/s, v_num=go-1, train/d_loss=17.60, train/g_loss=107.0, train/grid_loss=5.630, global_step=18.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mEpoch 1:  58%|█████▊    | 7/12 [01:07<00:48,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=970.0, train/grid_loss=46.00, global_step=19.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mEpoch 1:  67%|██████▋   | 8/12 [01:15<00:37,  0.11it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=970.0, train/grid_loss=46.00, global_step=19.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mEpoch 1:  67%|██████▋   | 8/12 [01:15<00:37,  0.11it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=112.0, train/grid_loss=5.220, global_step=20.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mcp checkpoint/dgmr_forecast18_ep50_max_nonzero0.9/global_step=10.0.ckpt s3://sagemaker-us-west-2-452145973879/checkpoints/dgmr_checkpoint05-14-2024-05-26-25/dgmr_forecast18_ep50_max_nonzero0.9/global_step=10.0.ckpt\u001b[0m\n",
      "\u001b[34mEpoch 1:  75%|███████▌  | 9/12 [01:28<00:29,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=112.0, train/grid_loss=5.220, global_step=20.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mEpoch 1:  75%|███████▌  | 9/12 [01:28<00:29,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=912.0, train/grid_loss=44.30, global_step=21.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 10/12 [01:36<00:19,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=912.0, train/grid_loss=44.30, global_step=21.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 10/12 [01:36<00:19,  0.10it/s, v_num=go-1, train/d_loss=27.40, train/g_loss=33.80, train/grid_loss=1.210, global_step=22.00, val/d_loss=2.520, val/g_loss=73.80, val/grid_loss=2.790]\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mValidation: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  20%|██        | 1/5 [00:02<00:10,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  40%|████      | 2/5 [00:05<00:07,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  80%|████████  | 4/5 [00:10<00:02,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 5/5 [00:12<00:00,  0.40it/s]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 10/12 [01:49<00:21,  0.09it/s, v_num=go-1, train/d_loss=27.40, train/g_loss=33.80, train/grid_loss=1.210, global_step=22.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 1:  92%|█████████▏| 11/12 [01:57<00:10,  0.09it/s, v_num=go-1, train/d_loss=27.40, train/g_loss=33.80, train/grid_loss=1.210, global_step=22.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 1:  92%|█████████▏| 11/12 [01:57<00:10,  0.09it/s, v_num=go-1, train/d_loss=4.800, train/g_loss=13.20, train/grid_loss=0.00653, global_step=23.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 12/12 [02:05<00:00,  0.10it/s, v_num=go-1, train/d_loss=4.800, train/g_loss=13.20, train/grid_loss=0.00653, global_step=23.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 12/12 [02:05<00:00,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=52.80, train/grid_loss=1.790, global_step=24.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 12/12 [02:05<00:00,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=52.80, train/grid_loss=1.790, global_step=24.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 1:   0%|          | 0/12 [00:00<?, ?it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=52.80, train/grid_loss=1.790, global_step=24.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:   0%|          | 0/12 [00:00<?, ?it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=52.80, train/grid_loss=1.790, global_step=24.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:   8%|▊         | 1/12 [00:08<01:33,  0.12it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=52.80, train/grid_loss=1.790, global_step=24.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:   8%|▊         | 1/12 [00:08<01:33,  0.12it/s, v_num=go-1, train/d_loss=42.80, train/g_loss=913.0, train/grid_loss=42.20, global_step=25.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:  17%|█▋        | 2/12 [00:16<01:20,  0.12it/s, v_num=go-1, train/d_loss=42.80, train/g_loss=913.0, train/grid_loss=42.20, global_step=25.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:  17%|█▋        | 2/12 [00:16<01:20,  0.12it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=379.0, train/grid_loss=16.90, global_step=26.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:  25%|██▌       | 3/12 [00:23<01:11,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=379.0, train/grid_loss=16.90, global_step=26.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:  25%|██▌       | 3/12 [00:23<01:11,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=110.0, train/grid_loss=4.120, global_step=27.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:  33%|███▎      | 4/12 [00:31<01:03,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=110.0, train/grid_loss=4.120, global_step=27.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:  33%|███▎      | 4/12 [00:31<01:03,  0.13it/s, v_num=go-1, train/d_loss=10.60, train/g_loss=-3.25, train/grid_loss=0.152, global_step=28.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:  42%|████▏     | 5/12 [00:39<00:55,  0.13it/s, v_num=go-1, train/d_loss=10.60, train/g_loss=-3.25, train/grid_loss=0.152, global_step=28.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mEpoch 2:  42%|████▏     | 5/12 [00:39<00:55,  0.13it/s, v_num=go-1, train/d_loss=4.250, train/g_loss=31.90, train/grid_loss=1.390, global_step=29.00, val/d_loss=33.80, val/g_loss=352.0, val/grid_loss=15.80]\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mValidation: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  20%|██        | 1/5 [00:02<00:10,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  40%|████      | 2/5 [00:05<00:07,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  80%|████████  | 4/5 [00:10<00:02,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 5/5 [00:12<00:00,  0.39it/s]\u001b[0m\n",
      "\u001b[34mEpoch 2:  42%|████▏     | 5/12 [00:52<01:13,  0.09it/s, v_num=go-1, train/d_loss=4.250, train/g_loss=31.90, train/grid_loss=1.390, global_step=29.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mEpoch 2:  50%|█████     | 6/12 [01:00<01:00,  0.10it/s, v_num=go-1, train/d_loss=4.250, train/g_loss=31.90, train/grid_loss=1.390, global_step=29.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mEpoch 2:  50%|█████     | 6/12 [01:00<01:00,  0.10it/s, v_num=go-1, train/d_loss=5.240, train/g_loss=1.18e+3, train/grid_loss=55.90, global_step=30.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mcp checkpoint/dgmr_forecast18_ep50_max_nonzero0.9/global_step=20.0.ckpt s3://sagemaker-us-west-2-452145973879/checkpoints/dgmr_checkpoint05-14-2024-05-26-25/dgmr_forecast18_ep50_max_nonzero0.9/global_step=20.0.ckpt\u001b[0m\n",
      "\u001b[34mEpoch 2:  58%|█████▊    | 7/12 [01:14<00:52,  0.09it/s, v_num=go-1, train/d_loss=5.240, train/g_loss=1.18e+3, train/grid_loss=55.90, global_step=30.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mEpoch 2:  58%|█████▊    | 7/12 [01:14<00:52,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=896.0, train/grid_loss=42.40, global_step=31.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mEpoch 2:  67%|██████▋   | 8/12 [01:21<00:40,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=896.0, train/grid_loss=42.40, global_step=31.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mEpoch 2:  67%|██████▋   | 8/12 [01:21<00:40,  0.10it/s, v_num=go-1, train/d_loss=1.810, train/g_loss=142.0, train/grid_loss=5.830, global_step=32.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mEpoch 2:  75%|███████▌  | 9/12 [01:29<00:29,  0.10it/s, v_num=go-1, train/d_loss=1.810, train/g_loss=142.0, train/grid_loss=5.830, global_step=32.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mEpoch 2:  75%|███████▌  | 9/12 [01:29<00:29,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=1.08e+3, train/grid_loss=52.40, global_step=33.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mEpoch 2:  83%|████████▎ | 10/12 [01:37<00:19,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=1.08e+3, train/grid_loss=52.40, global_step=33.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mEpoch 2:  83%|████████▎ | 10/12 [01:37<00:19,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=77.60, train/grid_loss=3.060, global_step=34.00, val/d_loss=18.30, val/g_loss=742.0, val/grid_loss=36.00]\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mValidation: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  20%|██        | 1/5 [00:02<00:10,  0.38it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  40%|████      | 2/5 [00:05<00:07,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  80%|████████  | 4/5 [00:10<00:02,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 5/5 [00:12<00:00,  0.39it/s]\u001b[0m\n",
      "\u001b[34mEpoch 2:  83%|████████▎ | 10/12 [01:50<00:22,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=77.60, train/grid_loss=3.060, global_step=34.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 2:  92%|█████████▏| 11/12 [01:58<00:10,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=77.60, train/grid_loss=3.060, global_step=34.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 2:  92%|█████████▏| 11/12 [01:58<00:10,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=85.40, train/grid_loss=3.530, global_step=35.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 12/12 [02:06<00:00,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=85.40, train/grid_loss=3.530, global_step=35.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 12/12 [02:06<00:00,  0.10it/s, v_num=go-1, train/d_loss=18.10, train/g_loss=-2.05, train/grid_loss=0.112, global_step=36.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 12/12 [02:06<00:00,  0.10it/s, v_num=go-1, train/d_loss=18.10, train/g_loss=-2.05, train/grid_loss=0.112, global_step=36.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 2:   0%|          | 0/12 [00:00<?, ?it/s, v_num=go-1, train/d_loss=18.10, train/g_loss=-2.05, train/grid_loss=0.112, global_step=36.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 3:   0%|          | 0/12 [00:00<?, ?it/s, v_num=go-1, train/d_loss=18.10, train/g_loss=-2.05, train/grid_loss=0.112, global_step=36.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 3:   8%|▊         | 1/12 [00:08<01:33,  0.12it/s, v_num=go-1, train/d_loss=18.10, train/g_loss=-2.05, train/grid_loss=0.112, global_step=36.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 3:   8%|▊         | 1/12 [00:08<01:33,  0.12it/s, v_num=go-1, train/d_loss=10.90, train/g_loss=842.0, train/grid_loss=39.70, global_step=37.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 3:  17%|█▋        | 2/12 [00:16<01:21,  0.12it/s, v_num=go-1, train/d_loss=10.90, train/g_loss=842.0, train/grid_loss=39.70, global_step=37.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 3:  17%|█▋        | 2/12 [00:16<01:21,  0.12it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=164.0, train/grid_loss=7.530, global_step=38.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 3:  25%|██▌       | 3/12 [00:23<01:11,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=164.0, train/grid_loss=7.530, global_step=38.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 3:  25%|██▌       | 3/12 [00:23<01:11,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=105.0, train/grid_loss=4.020, global_step=39.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 3:  33%|███▎      | 4/12 [00:31<01:03,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=105.0, train/grid_loss=4.020, global_step=39.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 3:  33%|███▎      | 4/12 [00:31<01:03,  0.13it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=14.50, train/grid_loss=0.142, global_step=40.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mcp checkpoint/dgmr_forecast18_ep50_max_nonzero0.9/global_step=30.0.ckpt s3://sagemaker-us-west-2-452145973879/checkpoints/dgmr_checkpoint05-14-2024-05-26-25/dgmr_forecast18_ep50_max_nonzero0.9/global_step=30.0.ckpt\u001b[0m\n",
      "\u001b[34mEpoch 3:  42%|████▏     | 5/12 [00:44<01:02,  0.11it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=14.50, train/grid_loss=0.142, global_step=40.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mEpoch 3:  42%|████▏     | 5/12 [00:44<01:02,  0.11it/s, v_num=go-1, train/d_loss=40.50, train/g_loss=25.50, train/grid_loss=1.450, global_step=41.00, val/d_loss=9.600, val/g_loss=817.0, val/grid_loss=39.60]\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mValidation: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  20%|██        | 1/5 [00:02<00:10,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  40%|████      | 2/5 [00:04<00:07,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  60%|██████    | 3/5 [00:07<00:04,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  80%|████████  | 4/5 [00:09<00:02,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 5/5 [00:12<00:00,  0.40it/s]\u001b[0m\n",
      "\u001b[34mEpoch 3:  42%|████▏     | 5/12 [00:57<01:21,  0.09it/s, v_num=go-1, train/d_loss=40.50, train/g_loss=25.50, train/grid_loss=1.450, global_step=41.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mEpoch 3:  50%|█████     | 6/12 [01:05<01:05,  0.09it/s, v_num=go-1, train/d_loss=40.50, train/g_loss=25.50, train/grid_loss=1.450, global_step=41.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mEpoch 3:  50%|█████     | 6/12 [01:05<01:05,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=1.15e+3, train/grid_loss=54.60, global_step=42.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mEpoch 3:  58%|█████▊    | 7/12 [01:13<00:52,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=1.15e+3, train/grid_loss=54.60, global_step=42.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mEpoch 3:  58%|█████▊    | 7/12 [01:13<00:52,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=623.0, train/grid_loss=28.80, global_step=43.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mEpoch 3:  67%|██████▋   | 8/12 [01:21<00:40,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=623.0, train/grid_loss=28.80, global_step=43.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mEpoch 3:  67%|██████▋   | 8/12 [01:21<00:40,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=159.0, train/grid_loss=6.010, global_step=44.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mEpoch 3:  75%|███████▌  | 9/12 [01:28<00:29,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=159.0, train/grid_loss=6.010, global_step=44.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mEpoch 3:  75%|███████▌  | 9/12 [01:28<00:29,  0.10it/s, v_num=go-1, train/d_loss=13.40, train/g_loss=794.0, train/grid_loss=37.70, global_step=45.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mEpoch 3:  83%|████████▎ | 10/12 [01:36<00:19,  0.10it/s, v_num=go-1, train/d_loss=13.40, train/g_loss=794.0, train/grid_loss=37.70, global_step=45.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mEpoch 3:  83%|████████▎ | 10/12 [01:36<00:19,  0.10it/s, v_num=go-1, train/d_loss=2.540, train/g_loss=32.50, train/grid_loss=1.100, global_step=46.00, val/d_loss=25.20, val/g_loss=36.00, val/grid_loss=2.170]\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mValidation: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  20%|██        | 1/5 [00:02<00:10,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  40%|████      | 2/5 [00:05<00:07,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  80%|████████  | 4/5 [00:10<00:02,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 5/5 [00:12<00:00,  0.39it/s]\u001b[0m\n",
      "\u001b[34mEpoch 3:  83%|████████▎ | 10/12 [01:49<00:21,  0.09it/s, v_num=go-1, train/d_loss=2.540, train/g_loss=32.50, train/grid_loss=1.100, global_step=46.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 3:  92%|█████████▏| 11/12 [01:57<00:10,  0.09it/s, v_num=go-1, train/d_loss=2.540, train/g_loss=32.50, train/grid_loss=1.100, global_step=46.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 3:  92%|█████████▏| 11/12 [01:57<00:10,  0.09it/s, v_num=go-1, train/d_loss=42.30, train/g_loss=9.540, train/grid_loss=0.00759, global_step=47.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 12/12 [02:05<00:00,  0.10it/s, v_num=go-1, train/d_loss=42.30, train/g_loss=9.540, train/grid_loss=0.00759, global_step=47.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 12/12 [02:05<00:00,  0.10it/s, v_num=go-1, train/d_loss=25.40, train/g_loss=25.20, train/grid_loss=0.103, global_step=48.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 12/12 [02:05<00:00,  0.10it/s, v_num=go-1, train/d_loss=25.40, train/g_loss=25.20, train/grid_loss=0.103, global_step=48.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 3:   0%|          | 0/12 [00:00<?, ?it/s, v_num=go-1, train/d_loss=25.40, train/g_loss=25.20, train/grid_loss=0.103, global_step=48.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 4:   0%|          | 0/12 [00:00<?, ?it/s, v_num=go-1, train/d_loss=25.40, train/g_loss=25.20, train/grid_loss=0.103, global_step=48.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 4:   8%|▊         | 1/12 [00:08<01:33,  0.12it/s, v_num=go-1, train/d_loss=25.40, train/g_loss=25.20, train/grid_loss=0.103, global_step=48.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 4:   8%|▊         | 1/12 [00:08<01:33,  0.12it/s, v_num=go-1, train/d_loss=23.90, train/g_loss=749.0, train/grid_loss=36.50, global_step=49.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 4:  17%|█▋        | 2/12 [00:16<01:21,  0.12it/s, v_num=go-1, train/d_loss=23.90, train/g_loss=749.0, train/grid_loss=36.50, global_step=49.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 4:  17%|█▋        | 2/12 [00:16<01:21,  0.12it/s, v_num=go-1, train/d_loss=22.20, train/g_loss=148.0, train/grid_loss=6.580, global_step=50.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mcp checkpoint/dgmr_forecast18_ep50_max_nonzero0.9/global_step=40.0.ckpt s3://sagemaker-us-west-2-452145973879/checkpoints/dgmr_checkpoint05-14-2024-05-26-25/dgmr_forecast18_ep50_max_nonzero0.9/global_step=40.0.ckpt\u001b[0m\n",
      "\u001b[34mEpoch 4:  25%|██▌       | 3/12 [00:29<01:28,  0.10it/s, v_num=go-1, train/d_loss=22.20, train/g_loss=148.0, train/grid_loss=6.580, global_step=50.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 4:  25%|██▌       | 3/12 [00:29<01:28,  0.10it/s, v_num=go-1, train/d_loss=5.520, train/g_loss=10.70, train/grid_loss=0.00868, global_step=51.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 4:  33%|███▎      | 4/12 [00:37<01:14,  0.11it/s, v_num=go-1, train/d_loss=5.520, train/g_loss=10.70, train/grid_loss=0.00868, global_step=51.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 4:  33%|███▎      | 4/12 [00:37<01:14,  0.11it/s, v_num=go-1, train/d_loss=5.550, train/g_loss=136.0, train/grid_loss=6.520, global_step=52.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 4:  42%|████▏     | 5/12 [00:45<01:03,  0.11it/s, v_num=go-1, train/d_loss=5.550, train/g_loss=136.0, train/grid_loss=6.520, global_step=52.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mEpoch 4:  42%|████▏     | 5/12 [00:45<01:03,  0.11it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=40.90, train/grid_loss=1.530, global_step=53.00, val/d_loss=8.780, val/g_loss=295.0, val/grid_loss=14.10]\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mValidation: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  20%|██        | 1/5 [00:02<00:10,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  40%|████      | 2/5 [00:05<00:07,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  80%|████████  | 4/5 [00:10<00:02,  0.40it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 5/5 [00:12<00:00,  0.39it/s]\u001b[0m\n",
      "\u001b[34mEpoch 4:  42%|████▏     | 5/12 [00:58<01:21,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=40.90, train/grid_loss=1.530, global_step=53.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mEpoch 4:  50%|█████     | 6/12 [01:06<01:06,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=40.90, train/grid_loss=1.530, global_step=53.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mEpoch 4:  50%|█████     | 6/12 [01:06<01:06,  0.09it/s, v_num=go-1, train/d_loss=18.90, train/g_loss=1.06e+3, train/grid_loss=51.40, global_step=54.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mEpoch 4:  58%|█████▊    | 7/12 [01:13<00:52,  0.09it/s, v_num=go-1, train/d_loss=18.90, train/g_loss=1.06e+3, train/grid_loss=51.40, global_step=54.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mEpoch 4:  58%|█████▊    | 7/12 [01:13<00:52,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=577.0, train/grid_loss=26.80, global_step=55.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mEpoch 4:  67%|██████▋   | 8/12 [01:21<00:40,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=577.0, train/grid_loss=26.80, global_step=55.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mEpoch 4:  67%|██████▋   | 8/12 [01:21<00:40,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=122.0, train/grid_loss=6.030, global_step=56.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mEpoch 4:  75%|███████▌  | 9/12 [01:29<00:29,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=122.0, train/grid_loss=6.030, global_step=56.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mEpoch 4:  75%|███████▌  | 9/12 [01:29<00:29,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=895.0, train/grid_loss=43.60, global_step=57.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mEpoch 4:  83%|████████▎ | 10/12 [01:37<00:19,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=895.0, train/grid_loss=43.60, global_step=57.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mEpoch 4:  83%|████████▎ | 10/12 [01:37<00:19,  0.10it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=451.0, train/grid_loss=20.50, global_step=58.00, val/d_loss=48.40, val/g_loss=484.0, val/grid_loss=22.30]\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mToo many dataloader workers: 8 (max is dataset.n_shards=4). Stopping 4 dataloader workers.\u001b[0m\n",
      "\u001b[34mValidation: |          | 0/? [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  20%|██        | 1/5 [00:02<00:10,  0.38it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  40%|████      | 2/5 [00:05<00:07,  0.38it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  80%|████████  | 4/5 [00:10<00:02,  0.39it/s]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 5/5 [00:12<00:00,  0.39it/s]\u001b[0m\n",
      "\u001b[34mEpoch 4:  83%|████████▎ | 10/12 [01:50<00:22,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=451.0, train/grid_loss=20.50, global_step=58.00, val/d_loss=13.70, val/g_loss=512.0, val/grid_loss=24.40]\u001b[0m\n",
      "\u001b[34mEpoch 4:  92%|█████████▏| 11/12 [01:58<00:10,  0.09it/s, v_num=go-1, train/d_loss=0.000, train/g_loss=451.0, train/grid_loss=20.50, global_step=58.00, val/d_loss=13.70, val/g_loss=512.0, val/grid_loss=24.40]\u001b[0m\n",
      "\u001b[34mEpoch 4:  92%|█████████▏| 11/12 [01:58<00:10,  0.09it/s, v_num=go-1, train/d_loss=12.00, train/g_loss=13.60, train/grid_loss=0.00758, global_step=59.00, val/d_loss=13.70, val/g_loss=512.0, val/grid_loss=24.40]\u001b[0m\n",
      "\u001b[34mEpoch 4: 100%|██████████| 12/12 [02:06<00:00,  0.10it/s, v_num=go-1, train/d_loss=12.00, train/g_loss=13.60, train/grid_loss=0.00758, global_step=59.00, val/d_loss=13.70, val/g_loss=512.0, val/grid_loss=24.40]\u001b[0m\n",
      "\u001b[34mEpoch 4: 100%|██████████| 12/12 [02:06<00:00,  0.10it/s, v_num=go-1, train/d_loss=22.30, train/g_loss=61.40, train/grid_loss=2.090, global_step=60.00, val/d_loss=13.70, val/g_loss=512.0, val/grid_loss=24.40]\u001b[0m\n",
      "\u001b[34mcp checkpoint/dgmr_forecast18_ep50_max_nonzero0.9/global_step=50.0.ckpt s3://sagemaker-us-west-2-452145973879/checkpoints/dgmr_checkpoint05-14-2024-05-26-25/dgmr_forecast18_ep50_max_nonzero0.9/global_step=50.0.ckpt\u001b[0m\n",
      "\u001b[34mEpoch 4: 100%|██████████| 12/12 [02:12<00:00,  0.09it/s, v_num=go-1, train/d_loss=22.30, train/g_loss=61.40, train/grid_loss=2.090, global_step=60.00, val/d_loss=13.70, val/g_loss=512.0, val/grid_loss=24.40]\u001b[0m\n",
      "\u001b[34m`Trainer.fit` stopped: `max_epochs=5` reached.\u001b[0m\n",
      "\u001b[34mEpoch 4: 100%|██████████| 12/12 [02:12<00:00,  0.09it/s, v_num=go-1, train/d_loss=22.30, train/g_loss=61.40, train/grid_loss=2.090, global_step=60.00, val/d_loss=13.70, val/g_loss=512.0, val/grid_loss=24.40]\u001b[0m\n",
      "\u001b[34malgo-1:371:2518 [2] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34malgo-1:375:2516 [6] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34malgo-1:372:2519 [3] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34malgo-1:376:2512 [7] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34malgo-1:373:2514 [4] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34malgo-1:374:2515 [5] NCCL INFO [Service thread] Connection closed by localRank 5\u001b[0m\n",
      "\u001b[34malgo-1:370:2517 [1] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34malgo-1:371:371 [2] NCCL INFO comm 0x5597b3a9eb00 rank 2 nranks 8 cudaDev 2 busId 180 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:375:375 [6] NCCL INFO comm 0x556ea2308e90 rank 6 nranks 8 cudaDev 6 busId 1c0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:372:372 [3] NCCL INFO comm 0x55f83426b090 rank 3 nranks 8 cudaDev 3 busId 190 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:376:376 [7] NCCL INFO comm 0x55b39cfaa7b0 rank 7 nranks 8 cudaDev 7 busId 1d0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:373:373 [4] NCCL INFO comm 0x5577d2ddcd20 rank 4 nranks 8 cudaDev 4 busId 1a0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:370:370 [1] NCCL INFO comm 0x56024f4ca4f0 rank 1 nranks 8 cudaDev 1 busId 170 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:374:374 [5] NCCL INFO comm 0x55c4a2bc6990 rank 5 nranks 8 cudaDev 5 busId 1b0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34mwandb: - Processing 8 runs with 8 files (0.17 MB/0.17 MB)\u001b[0m\n",
      "\u001b[34mwandb: \\ Processing 8 runs with 37 files (0.17 MB/0.30 MB)\u001b[0m\n",
      "\u001b[34mwandb: | Processing 8 runs with 43 files (0.24 MB/0.33 MB)\u001b[0m\n",
      "\u001b[34mwandb: / Processing 8 runs with 43 files (0.24 MB/0.33 MB)\u001b[0m\n",
      "\u001b[34mwandb: - Processing 8 runs with 43 files (0.33 MB/0.33 MB)\u001b[0m\n",
      "\u001b[34mwandb: \\ Processing 8 runs with 43 files (0.33 MB/0.33 MB)\u001b[0m\n",
      "\u001b[34mwandb: | Processing 8 runs with 43 files (0.33 MB/0.33 MB)\u001b[0m\n",
      "\u001b[34mwandb: / Processing 8 runs with 43 files (0.33 MB/0.33 MB)\u001b[0m\n",
      "\u001b[34mwandb: - Processing 8 runs with 48 files (0.33 MB/0.35 MB)\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:               epoch ▁▁▁▁▃▃▃▃▅▅▅▅▅▆▆▆▆█████\u001b[0m\n",
      "\u001b[34mwandb:         global_step ▁▂▂▃▄▄▅▅▆▇▇█\u001b[0m\n",
      "\u001b[34mwandb:        train/d_loss ▁▁▁▁█▂▁▁▃▅▁▅\u001b[0m\n",
      "\u001b[34mwandb:        train/g_loss ▁▂▂▂▆█▁▁▆▂▄▁\u001b[0m\n",
      "\u001b[34mwandb:     train/grid_loss ▁▂▂▂▆█▁▁▆▂▄▁\u001b[0m\n",
      "\u001b[34mwandb: trainer/global_step ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██\u001b[0m\n",
      "\u001b[34mwandb:          val/d_loss █▂▁▄▃▂▃▂▆▂\u001b[0m\n",
      "\u001b[34mwandb:          val/g_loss ▄▁▁▄▇█▁▃▅▅\u001b[0m\n",
      "\u001b[34mwandb:       val/grid_loss ▄▁▁▄▇█▁▃▅▅\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:               epoch 4\u001b[0m\n",
      "\u001b[34mwandb:         global_step 60.0\u001b[0m\n",
      "\u001b[34mwandb:        train/d_loss 22.29688\u001b[0m\n",
      "\u001b[34mwandb:        train/g_loss 61.40378\u001b[0m\n",
      "\u001b[34mwandb:     train/grid_loss 2.08817\u001b[0m\n",
      "\u001b[34mwandb: trainer/global_step 59\u001b[0m\n",
      "\u001b[34mwandb:          val/d_loss 13.74844\u001b[0m\n",
      "\u001b[34mwandb:          val/g_loss 511.80264\u001b[0m\n",
      "\u001b[34mwandb:       val/grid_loss 24.44571\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run dgmr-launch-2024-05-14-05-20-05-747-ukhysi-algo-1 at: https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-ukhysi-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at: https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20240514_052606-dgmr-launch-2024-05-14-05-20-05-747-ukhysi-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run dgmr-launch-2024-05-14-05-20-05-747-nt8f0n-algo-1 at: https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-nt8f0n-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at: https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-nt8f0n-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run dgmr-launch-2024-05-14-05-20-05-747-i9b66k-algo-1 at: https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-i9b66k-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at: https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-i9b66k-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run dgmr-launch-2024-05-14-05-20-05-747-rf8x5j-algo-1 at: https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-rf8x5j-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at: https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-rf8x5j-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run dgmr-launch-2024-05-14-05-20-05-747-v9n8q9-algo-1 at: https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-v9n8q9-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at: https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-v9n8q9-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run dgmr-launch-2024-05-14-05-20-05-747-8pv7l6-algo-1 at: https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-8pv7l6-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at: https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-8pv7l6-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run dgmr-launch-2024-05-14-05-20-05-747-0j9v6w-algo-1 at: https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-0j9v6w-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at: https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-0j9v6w-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run dgmr-launch-2024-05-14-05-20-05-747-ed9105-algo-1 at: https://wandb.ai/genai-view/dgmr-v2/runs/dgmr-launch-2024-05-14-05-20-05-747-ed9105-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at: https://wandb.ai/genai-view/dgmr-v2\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20240514_052635-dgmr-launch-2024-05-14-05-20-05-747-ed9105-algo-1/logs\u001b[0m\n",
      "\u001b[34malgo-1:96:2513 [0] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34malgo-1:96:96 [0] NCCL INFO comm 0x558c77749c20 rank 0 nranks 8 cudaDev 0 busId 160 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m2024-05-14 05:37:59,749 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-14 05:37:59,749 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-14 05:37:59,749 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-05-14 05:38:15 Uploading - Uploading generated training model\n",
      "2024-05-14 05:38:15 Completed - Training job completed\n",
      "Training seconds: 981\n",
      "Billable seconds: 981\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.2.0-gpu-py310-cu121-ubuntu20.04-sagemaker'\n",
    "\n",
    "instance_count = 1\n",
    "# instance_type = 'ml.p4d.24xlarge' ## p4d - 8*40G / p4de - 8*80G\n",
    "instance_type = 'ml.g5.48xlarge'\n",
    "\n",
    "environment = {\n",
    "    'NODE_NUMBER': str(instance_count),\n",
    "    'TRAIN_DATA_PATH': f's3://{bucket}/{prefix_data}/train/',\n",
    "    'VALID_DATA_PATH': f's3://{bucket}/{prefix_data}/valid/',\n",
    "    'PRETRAINED_MODEL_S3_PATH': f\"{input_model}/dgmr/\",\n",
    "    'OUTPUT_MODEL_S3_PATH': f's3://{bucket}/checkpoints/dgmr_checkpoint', # destination\n",
    "#    'LATEST_CHECKPOINT_S3_PATH': f's3://{bucket}/checkpoints/BrushNet_urbanic_random_custommask/checkpoint-60000/'\n",
    "    'WANDB_KEY': '0d32276b8b4b08bb83ecd160d941dba83b3b4975'\n",
    "}\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='entry.py',\n",
    "                      source_dir='./sm_scripts',\n",
    "                      base_job_name='dgmr-launch',\n",
    "                      instance_count=instance_count,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      max_run=3*24*3600, #任务最大存续时间，默认2day，需要提交ticket提升quota最大28天\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False)\n",
    "\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124e1c3-9f2f-4c6f-8f49-43311180565a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_dgmr_py310",
   "language": "python",
   "name": "conda_dgmr_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
